{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "with open('TrainLabels.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        if(i>0):\n",
    "            labels.append(row)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)[:,1]\n",
    "labels = labels.reshape(labels.shape[0],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped error file ERNData/Training/Data_S22_Sess05.csv\n"
     ]
    }
   ],
   "source": [
    "l = [2,6,7,11,12,13,14,16,17,18,20,21,22,23,24,26]\n",
    "l2 = [1,2,3,4,5]\n",
    "filenames = []\n",
    "for i in l:\n",
    "    for j in l2:\n",
    "        if i>9:\n",
    "            if(i == 22 and j == 5):\n",
    "                print(\"Skipped error file \" + \"ERNData/Training/Data_S\"+ str(i)+\"_Sess0\"+str(j)+\".csv\" )\n",
    "            else:\n",
    "                filenames.append(\"ERNData/Training/Data_S\"+ str(i)+\"_Sess0\"+str(j)+\".csv\")\n",
    "        else:\n",
    "            filenames.append(\"ERNData/Training/Data_S0\"+ str(i)+\"_Sess0\"+str(j)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n",
      "ERNData/Training/Data_S02_Sess01.csv\n",
      "(60, 260, 58)\n",
      "ERNData/Training/Data_S02_Sess02.csv\n",
      "(120, 260, 58)\n",
      "ERNData/Training/Data_S02_Sess03.csv\n",
      "(180, 260, 58)\n",
      "ERNData/Training/Data_S02_Sess04.csv\n",
      "(240, 260, 58)\n",
      "ERNData/Training/Data_S02_Sess05.csv\n",
      "(340, 260, 58)\n",
      "ERNData/Training/Data_S06_Sess01.csv\n",
      "(400, 260, 58)\n",
      "ERNData/Training/Data_S06_Sess02.csv\n",
      "(460, 260, 58)\n",
      "ERNData/Training/Data_S06_Sess03.csv\n",
      "(520, 260, 58)\n",
      "ERNData/Training/Data_S06_Sess04.csv\n",
      "(580, 260, 58)\n",
      "ERNData/Training/Data_S06_Sess05.csv\n",
      "(680, 260, 58)\n",
      "ERNData/Training/Data_S07_Sess01.csv\n",
      "(740, 260, 58)\n",
      "ERNData/Training/Data_S07_Sess02.csv\n",
      "(800, 260, 58)\n",
      "ERNData/Training/Data_S07_Sess03.csv\n",
      "(860, 260, 58)\n",
      "ERNData/Training/Data_S07_Sess04.csv\n"
     ]
    }
   ],
   "source": [
    "fb1 = []\n",
    "samples = []\n",
    "for filename in filenames:\n",
    "    print(np.shape(samples))\n",
    "    lis = []\n",
    "    print(filename)\n",
    "    with open(filename,'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if (i>0):\n",
    "                lis.append(row)\n",
    "            i = i + 1\n",
    "\n",
    "    a = np.array(lis)\n",
    "    b = a.astype(np.float)\n",
    "    #type(b[9599][58])\n",
    "\n",
    "    c = 0\n",
    "    for i in range(1,b.shape[0]):\n",
    "        if int(b[i][58]) == 1:\n",
    "            samples.append(b[i:i+260,1:59])\n",
    "            c += 1\n",
    "#             print(np.array(samples).shape)\n",
    "            #print(i,\" \",i/200)\n",
    "            fb1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(samples))\n",
    "goodSamples = samples.copy()\n",
    "print(np.shape(goodSamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(goodSamples))\n",
    "samples = goodSamples.copy()\n",
    "badSampleIndexes = []\n",
    "for index, thing in enumerate(samples):\n",
    "    shape = np.shape(thing)\n",
    "    if not shape == (260, 58):\n",
    "        print(shape)\n",
    "        badSampleIndexes.append(index)\n",
    "        \n",
    "for index in sorted(badSampleIndexes, reverse=True):\n",
    "    print(np.shape(samples[index]))\n",
    "    samples.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenFilenames = len(filenames)\n",
    "print(lenFilenames)\n",
    "len80PercentFilenames = round(lenFilenames*.8)\n",
    "print(len80PercentFilenames)\n",
    "for i in range(65, 80):\n",
    "    lis = []\n",
    "    print(i)\n",
    "    with open(filenames[i],'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if (i>0):\n",
    "                lis.append(row)\n",
    "            i = i + 1\n",
    "\n",
    "\n",
    "    a = np.array(lis)\n",
    "    b = a.astype(np.float)\n",
    "    \n",
    "    print(\"a\")\n",
    "    print(a)\n",
    "    \n",
    "    print(\"b\")\n",
    "    print(b)\n",
    "\n",
    "    #type(b[9599][58])\n",
    "\n",
    "    c = 0\n",
    "    print(np.shape(samples))\n",
    "    for i in range(1,b.shape[0]):\n",
    "        if int(b[i][58]) == 1:\n",
    "            print(\"in here\")\n",
    "            derp = b[i:i+260,1:59];\n",
    "            print(np.shape(derp))\n",
    "            print(derp)\n",
    "            samples.append(b[i:i+260,1:59])\n",
    "            print(np.shape(samples))\n",
    "            c += 1\n",
    "            #print(np.array(samples).shape)\n",
    "            #print(i,\" \",i/200)\n",
    "            fb1.append(i)\n",
    "\n",
    "    #s.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(samples))\n",
    "samples = np.array(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels.shape)\n",
    "\n",
    "print(np.shape((samples)))\n",
    "print(type(samples))\n",
    "print(np.shape((samples[0])))\n",
    "print(type((samples[0])))\n",
    "\n",
    "print(np.shape((samples[0][0])))\n",
    "print(type((samples[0][0])))\n",
    "\n",
    "print(samples[:2000][:][:])\n",
    "print(type(samples))\n",
    "train_samples = samples[:2000,:,:]\n",
    "val_samples = samples[2000:,:,:]\n",
    "train_labels = labels[:2000]\n",
    "val_labels = labels[2000:2480]\n",
    "\n",
    "print(\"train_samples.shape\",train_samples.shape)\n",
    "\n",
    "print(\"train_labels.shape\",train_labels.shape)\n",
    "\n",
    "print(\"val_samples.shape\",val_samples.shape)\n",
    "\n",
    "print(\"val_labels.shape\",val_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy\n",
    "\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_A_hold.shape,targets_A_hold.shape\n",
    "X_train = np.reshape(train_samples,[train_samples.shape[0],1,train_samples.shape[1],train_samples.shape[2]]).astype('float32')\n",
    "y_train = np.reshape(train_labels,[train_labels.shape[0],1]).astype('float32')\n",
    "\n",
    "X_val = np.reshape(val_samples,[val_samples.shape[0],1,val_samples.shape[1],val_samples.shape[2]]).astype('float32') \n",
    "y_val = np.reshape(val_labels,[val_labels.shape[0],1]).astype('float32')\n",
    "\n",
    "\n",
    "print (X_train.shape)\n",
    "\n",
    "print ('-------------')\n",
    "print (y_train.shape)\n",
    "\n",
    "print('------------------------------')\n",
    "print (X_val.shape)\n",
    "\n",
    "print ('-------------')\n",
    "print (y_val.shape)\n",
    "\n",
    "# (4000, 1, 260, 58)\n",
    "# -------------\n",
    "# (4000, 1)\n",
    "# ------------------------------\n",
    "# (1340, 1, 260, 58)\n",
    "# -------------\n",
    "# (1340, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val[:480]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:,:,:,:56]\n",
    "X_val = X_val[:,:,:,:56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1,56), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints. \n",
    "        #4, 2, 487\n",
    "        self.fc1 = nn.Linear(4*2*16, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "      #  x = x.double()\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling2(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling3(x)\n",
    "        print (x.size())\n",
    "        # FC Layer\n",
    "        x = x.view(-1, 4*2*16)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = EEGNet()#.cuda(0)\n",
    "#print (net.forward(Variable(torch.Tensor(np.random.rand(1, 1, 120, 64)))))#.cuda(0))))\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, Y, params = [\"acc\"]):\n",
    "    results = []\n",
    "    batch_size = 2\n",
    "    \n",
    "    predicted = []\n",
    "   \n",
    "    inputs = Variable(torch.from_numpy(X))#.cuda(0))\n",
    "    predicted = model(inputs)\n",
    "\n",
    "    predicted = predicted.data.cpu().numpy()\n",
    "    \n",
    "    for param in params:\n",
    "        if param == 'acc':\n",
    "            results.append(accuracy_score(Y, np.round(predicted)))\n",
    "        if param == \"auc\":\n",
    "            results.append(roc_auc_score(Y, predicted))\n",
    "        if param == \"recall\":\n",
    "            results.append(recall_score(Y, np.round(predicted)))\n",
    "        if param == \"precision\":\n",
    "            results.append(precision_score(Y, np.round(predicted)))\n",
    "        if param == \"fmeasure\":\n",
    "            precision = precision_score(Y, np.round(predicted))\n",
    "            recall = recall_score(Y, np.round(predicted))\n",
    "            results.append(2*precision*recall/ (precision+recall))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "        running_loss=0.0\n",
    "        # Mini batch training\n",
    "        for i in range(0,len(X_train),batch_size):       \n",
    "            #print ('Epocj,mini-batch',epoch,i)\n",
    "            inputs = torch.from_numpy(X_train[i:i+batch_size])\n",
    "            labels = torch.FloatTensor(y_train[i:i+batch_size]*1.0)\n",
    "\n",
    "           # print (labels.size())\n",
    "            # wrap them in Variable\n",
    "            #inputs, labels = Variable(inputs.cuda(0)), Variable(labels.cuda(0))\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            #print ('op',outputs.size())\n",
    "            #print('labels',labels.size())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            lossNumber = loss.data.item()\n",
    "\n",
    "            running_loss += lossNumber\n",
    "             # for graphing puposes\n",
    "            running_loss_array.append(lossNumber)\n",
    "        \n",
    "        return running_loss_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(net,epoch):\n",
    "    \n",
    "    predicted_loss=[]\n",
    "   \n",
    "    inputs = torch.from_numpy(X_val)\n",
    "    labels = torch.FloatTensor(y_val*1.0)\n",
    "\n",
    "    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "    '''\n",
    "    output = net(data)\n",
    "    valid_loss = valid_loss + F.nll_loss(output, target, size_average=False).data[0] \n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    correct = correct + pred.eq(target.data.view_as(pred)).cpu().sum() \n",
    "    '''\n",
    "    ########################        \n",
    "    results = []\n",
    "   \n",
    "    predicted = []\n",
    "   \n",
    "    inputs = Variable(torch.from_numpy(X_val))#.cuda(0))\n",
    "    predicted = net(inputs)\n",
    "\n",
    "    predicted = predicted.data.cpu().numpy()\n",
    "    #from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "   # print ('predicted',predicted.shape)\n",
    "    Y=labels.data.numpy()\n",
    "    \n",
    "    for param in [\"acc\", \"auc\", \"recall\", \"precision\",\"fmeasure\"]:\n",
    "        if param == 'acc':\n",
    "            results.append(accuracy_score(Y, np.round(predicted)))\n",
    "        if param == \"auc\":\n",
    "            results.append(roc_auc_score(Y, predicted))\n",
    "        if param == \"recall\":\n",
    "            results.append(recall_score(Y, np.round(predicted)))\n",
    "        if param == \"precision\":\n",
    "            results.append(precision_score(Y, np.round(predicted)))\n",
    "        if param == \"fmeasure\":\n",
    "            precision = precision_score(Y, np.round(predicted))\n",
    "            recall = recall_score(Y, np.round(predicted))\n",
    "            results.append(2*precision*recall/ (precision+recall))\n",
    "     \n",
    "   # print (len(results))\n",
    "    return results #validation_loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "running_loss_array=[]\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    \n",
    "    print (\"\\nEpoch \", epoch)\n",
    "   # print ('range',int(len(X_train)/batch_size-1))\n",
    "    train_loss=train(epoch)\n",
    "    valid_results=valid(net,epoch)\n",
    "    print ('Parameters:[\"acc\", \"auc\", \"recall\", \"precision\",\"fmeasure\"]')\n",
    "    print ('validation_results',valid_results)\n",
    "    # Validation accuracy\n",
    "    #print (\"Running Training Loss \", running_loss)\n",
    "   # print (\"Train - \", evaluate(net, X_train, y_train, params))\n",
    "   # print (\"Validation - \", evaluate(net, X_val, y_val, params))\n",
    "    #print (\"Test - \", evaluate(net, X_test, y_test, params))\n",
    "\n",
    "#plot loss\n",
    "plt.plot(train_loss)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
